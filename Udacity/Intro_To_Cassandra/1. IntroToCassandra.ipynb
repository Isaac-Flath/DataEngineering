{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ETL Pipeline for Pre-Processing the Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages \n",
    "import pandas as pd\n",
    "import cassandra\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of filepaths to process original event csv data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/home\n"
     ]
    }
   ],
   "source": [
    "# checking your current working directory\n",
    "print(os.getcwd())\n",
    "\n",
    "# Get your current folder and subfolder event data\n",
    "filepath = os.getcwd() + '/event_data'\n",
    "\n",
    "# Create a for loop to create a list of files and collect each filepath\n",
    "for root, dirs, files in os.walk(filepath):\n",
    "    \n",
    "# join the file path and roots with the subdirectories using glob\n",
    "    file_path_list = glob.glob(os.path.join(root,'*'))\n",
    "    #print(file_path_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the files to create the data file csv that will be used for Apache Casssandra tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiating an empty list of rows that will be generated from each file\n",
    "full_data_rows_list = [] \n",
    "    \n",
    "# for every filepath in the file path list \n",
    "for f in file_path_list:\n",
    "\n",
    "# reading csv file \n",
    "    with open(f, 'r', encoding = 'utf8', newline='') as csvfile: \n",
    "        # creating a csv reader object \n",
    "        csvreader = csv.reader(csvfile) \n",
    "        next(csvreader)\n",
    "        \n",
    " # extracting each data row one by one and append it        \n",
    "        for line in csvreader:\n",
    "            #print(line)\n",
    "            full_data_rows_list.append(line) \n",
    "            \n",
    "# uncomment the code below if you would like to get total number of rows \n",
    "#print(len(full_data_rows_list))\n",
    "# uncomment the code below if you would like to check to see what the list of event data rows will look like\n",
    "#print(full_data_rows_list)\n",
    "\n",
    "# creating a smaller event data csv file called event_datafile_full csv that will be used to insert data into the \\\n",
    "# Apache Cassandra tables\n",
    "csv.register_dialect('myDialect', quoting=csv.QUOTE_ALL, skipinitialspace=True)\n",
    "\n",
    "with open('event_datafile_new.csv', 'w', encoding = 'utf8', newline='') as f:\n",
    "    writer = csv.writer(f, dialect='myDialect')\n",
    "    writer.writerow(['artist','firstName','gender','itemInSession','lastName','length',\\\n",
    "                'level','location','sessionId','song','userID'])\n",
    "    for row in full_data_rows_list:\n",
    "        if (row[0] == ''):\n",
    "            continue\n",
    "        writer.writerow((row[0], row[2], row[3], row[4], row[5], row[6], row[7], row[8], row[12], row[13], row[16]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6821\n"
     ]
    }
   ],
   "source": [
    "# check the number of rows in your csv file\n",
    "with open('event_datafile_new.csv', 'r', encoding = 'utf8') as f:\n",
    "    print(sum(1 for line in f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Apache Cassandra Modeling\n",
    "\n",
    "## Data Inputs\n",
    "\n",
    "The event_datafile_new.csv contains the following columns: \n",
    "\n",
    "- artist \n",
    "- firstName of user\n",
    "- gender of user\n",
    "- item number in session\n",
    "- last name of user\n",
    "- length of the song\n",
    "- level (paid or free song)\n",
    "- location of the user\n",
    "- sessionId\n",
    "- song title\n",
    "- userID\n",
    "\n",
    "The image below is a screenshot of what the denormalized data should appear like in the <font color=red>**event_datafile_new.csv**</font> after the code above is run:<br>\n",
    "\n",
    "<img src=\"images/image_event_datafile_new.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Cassandra Connection and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should make a connection to a Cassandra instance your local machine \n",
    "# (127.0.0.1)\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "cluster = Cluster()\n",
    "session = cluster.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f1630e4f7f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = '''\n",
    "CREATE KEYSPACE IF NOT EXISTS udacity\n",
    "WITH REPLICATION = {\n",
    "    'class' : 'SimpleStrategy',\n",
    "    'replication_factor' : 1\n",
    "    }\n",
    "'''\n",
    "session.execute(qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Keyspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.set_keyspace('udacity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "\n",
    "We define a few helper functions that will be used later on to remove some of the redudant work neeeded for modeling for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data Column Names\n",
    "file = 'event_datafile_new.csv'\n",
    "\n",
    "fcols = [\"artist\",\"firstName\",\"gender\",\"itemInSession\",\"lastName\",\"length\",\"level\",\"location\",\"sessionID\",\"song\",\"userID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an insert query\n",
    "def get_insert_query_base(table, cols):\n",
    "    \"\"\"\n",
    "    Generate a base SQL INSERT query string for a given table and column list.\n",
    "\n",
    "    Parameters:\n",
    "    table (str): The name of the SQL table to insert data into.\n",
    "    cols (list): A list of column names to specify in the INSERT statement.\n",
    "\n",
    "    Returns:\n",
    "    str: A formatted SQL INSERT query string that can be used to insert data into the specified table\n",
    "         using placeholders for column values.\n",
    "\n",
    "    Example:\n",
    "    >>> get_insert_query_base(\"employees\", [\"id\", \"name\", \"salary\"])\n",
    "    'INSERT INTO employees (id, name, salary) VALUES (%s, %s, %s)'\n",
    "\n",
    "    Note:\n",
    "    - This function returns a base query string with placeholders for column values. You should use\n",
    "      this as a template and add the actual values using a database cursor's execute method.\n",
    "    - The column names provided in the 'cols' parameter should be in the same order as the values\n",
    "      you intend to insert into the corresponding columns.\n",
    "    - The function uses the %s placeholder for values, which is common in database libraries like cassandra. \n",
    "      Make sure to adapt it to the specific placeholder syntax of your database library if necessary.\n",
    "    \"\"\"\n",
    "    \n",
    "    return f'''INSERT INTO {table} {str(cols).replace(\"'\",'')} VALUES ({('%s,'*len(cols))[:-1]})'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an insert query for a table based on columns and data types\n",
    "def get_insert_query(file, table, cols, dtypes):\n",
    "    \"\"\"\n",
    "    Generate and execute an SQL INSERT query using data from a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "    - file (str): The path to the CSV file containing the data to be inserted.\n",
    "    - table (str): The name of the database table where the data will be inserted.\n",
    "    - cols (list of str): A list of column names in the table corresponding to the CSV data.\n",
    "    - dtypes (list of callable): A list of callable functions (e.g., int, str) that will be used\n",
    "      to cast and format the CSV data before insertion.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "\n",
    "    This function reads data from the specified CSV file and constructs an SQL INSERT query for\n",
    "    inserting that data into the specified database table. The function uses the 'cols' list to\n",
    "    map the CSV columns to the corresponding table columns, and the 'dtypes' list to specify\n",
    "    how the data should be converted to the appropriate data types for insertion.\n",
    "\n",
    "    Example:\n",
    "    - Suppose we have a CSV file 'data.csv' with columns 'name', 'age', and 'city', and we want\n",
    "      to insert this data into a table 'persons' with corresponding columns 'full_name', 'age',\n",
    "      and 'residence'. We can call the function as follows:\n",
    "\n",
    "      get_insert_query('data.csv', 'persons', ['full_name', 'age', 'residence'], [str, int, str])\n",
    "\n",
    "    This will generate and execute an SQL INSERT query that inserts the data from 'data.csv'\n",
    "    into the 'persons' table, converting 'age' to an integer and leaving 'name' and 'city' as\n",
    "    strings.\n",
    "    \"\"\"\n",
    "    query = get_insert_query_base(table,cols)\n",
    "    with open(file, encoding = 'utf8') as f:\n",
    "        csvreader = csv.reader(f)\n",
    "        next(csvreader)\n",
    "        for line in csvreader:\n",
    "            data = tuple(d(line[fcols.index(c)]) for c,d in zip(cols, dtypes))\n",
    "            session.execute(query, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretty table to display query results\n",
    "from prettytable import PrettyTable\n",
    "def display_rows(rows,cols):\n",
    "    \"\"\"\n",
    "    Display a tabular representation of data rows.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rows : list of lists\n",
    "        A list of rows where each row is represented as a list of values.\n",
    "    cols : list of str\n",
    "        A list of column headers as strings. The number of columns should match\n",
    "        the number of elements in each row.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function does not return a value but prints the tabular data.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> data = [\n",
    "    ...     [\"Alice\", 25, \"Engineer\"],\n",
    "    ...     [\"Bob\", 30, \"Manager\"],\n",
    "    ...     [\"Charlie\", 28, \"Designer\"]\n",
    "    ... ]\n",
    "    >>> headers = [\"Name\", \"Age\", \"Occupation\"]\n",
    "    >>> display_rows(data, headers)\n",
    "    +---------+-----+------------+\n",
    "    |   Name  | Age | Occupation |\n",
    "    +---------+-----+------------+\n",
    "    |  Alice  |  25 |  Engineer  |\n",
    "    |   Bob   |  30 |  Manager   |\n",
    "    | Charlie |  28 |  Designer  |\n",
    "    +---------+-----+------------+\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the PrettyTable library to create a formatted table\n",
    "    for displaying the data. Make sure to install PrettyTable using\n",
    "    `pip install PrettyTable` before using this function.\n",
    "    \"\"\"\n",
    "    t = PrettyTable(cols)\n",
    "    for row in rows:\n",
    "        t.add_row(row)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1:  \n",
    "\n",
    "The goal of query 1 is to get information about songs (artist, title, length) for songs that have been heard.  It's important for us to be able to isolate this information for particular sessions (`sessionID`), and when they happened in that session (`itemInSession`).\n",
    "\n",
    "In order to accomplish this we have a query specific things to consider when data modeling:\n",
    "+ The columns that are neccesary are at minimum artist, title, length, sessionID, and itemInSession.  The requirements use those in either the output or filter criteria so we know those must be in the table.\n",
    "+ sessionsID and itemInSession must be part of a primary, composite, or clustering key because those are used as filters in the target query.\n",
    "\n",
    "With those requirements in mind we can construct a create table statement.  Once we have that we need to verify that the primary + composite key combinations are unique.  In this case they are, so we do not have to do anything more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f1630ec1e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = '''\n",
    "CREATE TABLE session_songs (\n",
    "    sessionID int,\n",
    "    itemInSession int,\n",
    "    artist text,\n",
    "    song text,\n",
    "    length decimal,\n",
    "    PRIMARY KEY(sessionID,itemInSession)\n",
    ")\n",
    "'''\n",
    "session.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = ('sessionID','itemInSession','artist','song','length')\n",
    "dtypes = (int, int, str, str, float)\n",
    "\n",
    "get_insert_query(file, 'session_songs', cols, dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+----------+\n",
      "|   artist  |               song              |  length  |\n",
      "+-----------+---------------------------------+----------+\n",
      "| Faithless | Music Matters (Mark Knight Dub) | 495.3073 |\n",
      "+-----------+---------------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "qry = 'select artist, song, length from session_songs where sessionId = 338 and itemInSession = 4;'\n",
    "rows = session.execute(qry)\n",
    "display_rows(rows,('artist', 'song', 'length'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2\n",
    "\n",
    "\n",
    "The goal of query 2 is to get information about what songs users listed (song, user name) for songs they listened to.  It's important for us to be able to isolate this information for particular users (`userID`) and a given session (`sessionID`).\n",
    "\n",
    "In order to accomplish this we have a query specific things to consider when data modeling:\n",
    "+ The columns that are neccesary are at minimum artist, song, firstName, lastName, userID, sessionID, and ItemInSession.  The requirements use those in either the output, filter, or sort criteria so we know those must be in the table.\n",
    "+ userID and sessionID must be part of a primary, composite, or clustering key because those are used as filters in the target query.\n",
    "+ The query should be sorted by ItemInSession.  For optimal performance, we can make ItemInSession a clustering key so that the data is sorted by default.\n",
    "\n",
    "With those requirements in mind we can construct a create table statement.  Once we have that we need to verify that the primary + composite key combinations are unique.  In this case they are, so we do not have to do anything more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f1630e93a90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = '''\n",
    "CREATE TABLE user_artists (\n",
    "    userID int,\n",
    "    sessionID int,\n",
    "    itemInSession int,\n",
    "    artist text,\n",
    "    song text,\n",
    "    firstname text,\n",
    "    lastname text,\n",
    "    PRIMARY KEY((userID, sessionID), itemInSession)\n",
    ")\n",
    "'''\n",
    "session.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = ('userID', 'sessionID', 'itemInSession', 'artist','song','firstName','lastName')\n",
    "dtypes = (int, int, int, str, str, str, str)\n",
    "\n",
    "get_insert_query(file, 'user_artists', cols, dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------------------------------------------+-----------+----------+\n",
      "|       artist      |                         song                         | firstName | lastName |\n",
      "+-------------------+------------------------------------------------------+-----------+----------+\n",
      "|  Down To The Bone |                  Keep On Keepin' On                  |   Sylvie  |   Cruz   |\n",
      "|    Three Drives   |                     Greece 2000                      |   Sylvie  |   Cruz   |\n",
      "| Sebastien Tellier |                      Kilometer                       |   Sylvie  |   Cruz   |\n",
      "|   Lonnie Gordon   | Catch You Baby (Steve Pitron & Max Sanna Radio Edit) |   Sylvie  |   Cruz   |\n",
      "+-------------------+------------------------------------------------------+-----------+----------+\n"
     ]
    }
   ],
   "source": [
    "qry = 'select artist, song, firstName, lastName from user_artists where userID = 10 and sessionID = 182'\n",
    "rows = session.execute(qry)\n",
    "display_rows(rows,('artist', 'song', 'firstName', 'lastName'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 3: Give me every user name (first and last) in my music app history who listened to the song 'All Hands Against His Own'\n",
    "\n",
    "The goal of query 3 is to get users that have listened to a particular song at any point.\n",
    "\n",
    "In order to accomplish this we have a query specific things to consider when data modeling:\n",
    "+ The columns that are neccesary are at minimum firsName, lastName, and song.  The requirements use those in either the output, filter, or sort criteria so we know those must be in the table.\n",
    "+ song must be part of the primary or clustering key as it is used as a filter requirement.\n",
    "\n",
    "With those requirements in mind we can construct a create table statement.  Once we have that we need to verify that the primary + composite key combinations are unique.  In this case song is not sufficient as a primary key as it is not unique, and so we need to add userID to make the primary and clustering key combination unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<cassandra.cluster.ResultSet at 0x7f1630db9f60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qry = '''\n",
    "CREATE TABLE song_users (\n",
    "    song text,\n",
    "    userID int,\n",
    "    firstName text,\n",
    "    lastName text,\n",
    "    itemInSession int,\n",
    "    sessionID int,\n",
    "    PRIMARY KEY(song, userID)\n",
    ")\n",
    "'''\n",
    "session.execute(qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = ('song','userID','firstName','lastName','itemInSession','sessionID')\n",
    "dtypes = (str, int, str, str, int, int)\n",
    "\n",
    "get_insert_query(file, 'song_users', cols, dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "| firstName  | lastName |\n",
      "+------------+----------+\n",
      "| Jacqueline |  Lynch   |\n",
      "|   Tegan    |  Levine  |\n",
      "|    Sara    | Johnson  |\n",
      "+------------+----------+\n"
     ]
    }
   ],
   "source": [
    "qry = \"select firstName, lastName from song_users where song = 'All Hands Against His Own'\"\n",
    "rows = session.execute(qry)\n",
    "display_rows(rows,('firstName', 'lastName'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the tables before closing out the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for table in ('session_songs','user_artists','song_users'):\n",
    "    session.execute(f\"DROP TABLE IF EXISTS {table}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the session and cluster connectionÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.shutdown()\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
